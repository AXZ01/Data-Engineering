{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d1016d9-b709-4960-81fa-5472714ae4e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------+------------+--------------+-----------+------+-----------+-------------+-------------+-----------+-------+------------+----------------+------------------+\n|Customer_ID|Age|Gender|  Occupation|Marital Status|Family Size|Income|Expenditure|Use Frequency|Loan Category|Loan Amount|Overdue| Debt Record| Returned Cheque| Dishonour of Bill|\n+-----------+---+------+------------+--------------+-----------+------+-----------+-------------+-------------+-----------+-------+------------+----------------+------------------+\n|    IB14001| 30|  MALE|BANK MANAGER|        SINGLE|          4| 50000|      22199|            6|      HOUSING| 10,00,000 |      5|      42,898|               6|                 9|\n|    IB14008| 44|  MALE|   PROFESSOR|       MARRIED|          6| 51000|      19999|            4|     SHOPPING|     50,000|      3|      33,999|               1|                 5|\n|    IB14012| 30|FEMALE|     DENTIST|        SINGLE|          3| 58450|      27675|            5|   TRAVELLING|     75,000|      6|      20,876|               3|                 1|\n|    IB14018| 29|  MALE|     TEACHER|       MARRIED|          5| 45767|      12787|            3|    GOLD LOAN|  6,00,000 |      7|      11,000|               0|                 4|\n|    IB14022| 34|  MALE|      POLICE|        SINGLE|          4| 43521|      11999|            3|   AUTOMOBILE|  2,00,000 |      2|      43,898|               1|                 2|\n+-----------+---+------+------------+--------------+-----------+------+-----------+-------------+-------------+-----------+-------+------------+----------------+------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initializing SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Loan Data Processing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# File path\n",
    "file_path = \"/FileStore/tables/loan-1.csv\"\n",
    "\n",
    "# Load the CSV file into a PySpark DataFrame\n",
    "loans_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "# Display the DataFrame to verify\n",
    "loans_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "150273af-8299-484a-ac1a-bf687eb685cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "loans_df.createOrReplaceTempView(\"challenge_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df0c2ff3-aab6-4b51-ace9-2331151b2c43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n|Customer_ID|Age|\n+-----------+---+\n|    IB14008| 44|\n|    IB14022| 34|\n|    IB14024| 55|\n|    IB14025| 39|\n|    IB14027| 51|\n|    IB14031| 37|\n|    IB14034| 32|\n|    IB14037| 54|\n|    IB14039| 45|\n|    IB14041| 59|\n|    IB14045| 31|\n|    IB14049| 49|\n|    IB14050| 56|\n|    IB14054| 58|\n|    IB14060| 36|\n|    IB14070| 40|\n|    IB14078| 45|\n|    IB14082| 60|\n|    IB14086| 51|\n|    IB14092| 47|\n+-----------+---+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "age_above_30 = spark.sql(\"\"\"\n",
    "    SELECT Customer_ID, Age\n",
    "    FROM challenge_data\n",
    "    WHERE Age > 30\n",
    "\"\"\")\n",
    "age_above_30.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "094d7392-4f40-457c-84ac-fae36e3353d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-----------+\n|Customer_ID|Income|Family Size|\n+-----------+------+-----------+\n|    IB14001| 50000|          4|\n|    IB14008| 51000|          6|\n|    IB14018| 45767|          5|\n|    IB14022| 43521|          4|\n|    IB14024| 34999|          6|\n|    IB14025| 46619|          6|\n|    IB14031| 55999|          5|\n|    IB14032| 60111|          4|\n|    IB14037| 48099|          5|\n|    IB14039| 45777|          7|\n|    IB14041| 50999|          4|\n|    IB14042| 60111|          4|\n|    IB14045| 40999|          5|\n|    IB14049| 45999|          4|\n|    IB14054| 60000|          5|\n|    IB14057| 40000|          4|\n|    IB14060| 35000|          4|\n|    IB14070| 38000|          4|\n|    IB14078| 40000|          4|\n|    IB14082| 70000|          5|\n+-----------+------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "income_family_filter = spark.sql(\"\"\"\n",
    "    SELECT Customer_ID, Income, `Family Size`\n",
    "    FROM challenge_data\n",
    "    WHERE Income > 30000 AND `Family Size` > 3\n",
    "\"\"\")\n",
    "\n",
    "income_family_filter.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c520fa1-5f0f-40f6-8090-ecf51820f0d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n|unique_occupations|\n+------------------+\n|                39|\n+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "occupation_count_df = spark.sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT Occupation) AS unique_occupations\n",
    "    FROM challenge_data\n",
    "\"\"\")\n",
    "\n",
    "occupation_count_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8015249-d9c1-4e29-bd60-74fc81db7090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|   average_income|\n+-----------------+\n|68339.49145299145|\n+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "avg_income_df = spark.sql(\"\"\"\n",
    "    SELECT AVG(Income) AS average_income\n",
    "    FROM challenge_data\n",
    "\"\"\")\n",
    "\n",
    "avg_income_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52307dbd-3801-43bc-9f68-f911d05e5751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|max_loan|\n+--------+\n| 999,698|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "max_loan_df = spark.sql(\"\"\"\n",
    "    SELECT MAX(`Loan Amount`) AS max_loan\n",
    "    FROM challenge_data\n",
    "\"\"\")\n",
    "\n",
    "max_loan_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a15a0f-f19e-4d17-ad64-6728e13086a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|  min_loan|\n+----------+\n| 1,00,000 |\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "min_loan_df = spark.sql(\"\"\"\n",
    "    SELECT MIN(`Loan Amount`) AS min_loan\n",
    "    FROM challenge_data\n",
    "\"\"\")\n",
    "\n",
    "min_loan_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bffa5f86-8b3f-4d5e-b4bd-b976b0fcea3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|total_expenditure|\n+-----------------+\n|         13243460|\n+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "total_expenditure_df = spark.sql(\"\"\"\n",
    "    SELECT SUM(Expenditure) AS total_expenditure\n",
    "    FROM challenge_data\n",
    "\"\"\")\n",
    "\n",
    "total_expenditure_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b873efa-1255-4de9-bc5d-d6adda771c34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+\n|     Loan Category|num_customers|\n+------------------+-------------+\n|           HOUSING|           67|\n|        TRAVELLING|           53|\n|       BOOK STORES|            7|\n|       AGRICULTURE|           12|\n|         GOLD LOAN|           77|\n|  EDUCATIONAL LOAN|           20|\n|        AUTOMOBILE|           60|\n|          BUSINESS|           24|\n|COMPUTER SOFTWARES|           35|\n|           DINNING|           14|\n|          SHOPPING|           35|\n|       RESTAURANTS|           41|\n|       ELECTRONICS|           14|\n|          BUILDING|            7|\n|        RESTAURANT|           20|\n|   HOME APPLIANCES|           14|\n+------------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "loan_category_count_df = spark.sql(\"\"\"\n",
    "    SELECT `Loan Category`, COUNT(Customer_ID) AS num_customers\n",
    "    FROM challenge_data\n",
    "    GROUP BY `Loan Category`\n",
    "\"\"\")\n",
    "\n",
    "loan_category_count_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cd572f2-b44d-440d-bc51-f6aaf4a1494e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n|          Occupation|        avg_income|\n+--------------------+------------------+\n|      CIVIL ENGINEER|60359.666666666664|\n|     FIRE DEPARTMENT|55357.916666666664|\n|          ACCOUNTANT| 56623.28571428572|\n|        BANK MANAGER|           92191.0|\n|      SYSTEM OFFICER|           56780.0|\n|           NUTRITION|           55650.0|\n|           DIETICIAN| 72599.16666666667|\n|               CLERK|         76871.125|\n|   SOFTWARE ENGINEER|           61107.8|\n|AGRICULTURAL ENGI...|         82060.625|\n|   ASSISTANT MANAGER|54866.166666666664|\n|             TEACHER| 52812.73333333333|\n| ASSISTANT PROFESSOR|53319.333333333336|\n|     SYSTEM ENGINEER|60509.333333333336|\n| CHARTERED APPRAISER| 76456.72727272728|\n|                NAVY|        71190.9375|\n|              POLICE| 49049.88888888889|\n|            BUSINESS|        56682.5625|\n|              FARMER| 74906.85714285714|\n|              DRIVER|64450.833333333336|\n+--------------------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "avg_income_by_occupation_df = spark.sql(\"\"\"\n",
    "    SELECT Occupation, AVG(Income) AS avg_income\n",
    "    FROM challenge_data\n",
    "    GROUP BY Occupation\n",
    "\"\"\")\n",
    "\n",
    "avg_income_by_occupation_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e26b733-19d7-4d1a-8185-b68271abeb53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+----------+\n|Marital Status|max_loan|  min_loan|\n+--------------+--------+----------+\n|       MARRIED| 999,698| 1,02,256 |\n|        SINGLE| 964,109| 1,00,000 |\n+--------------+--------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "loan_amount_by_marital_status_df = spark.sql(\"\"\"\n",
    "    SELECT `Marital Status`, MAX(`Loan Amount`) AS max_loan, MIN(`Loan Amount`) AS min_loan\n",
    "    FROM challenge_data\n",
    "    GROUP BY `Marital Status`\n",
    "\"\"\")\n",
    "\n",
    "loan_amount_by_marital_status_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05711e84-d48f-471b-8c4c-15d2d72607d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-----------------+---------------+\n|Family Size|        avg_income|total_loan_amount|max_expenditure|\n+-----------+------------------+-----------------+---------------+\n|          6|59968.545454545456|             NULL|          62541|\n|          3| 71669.28787878787|             NULL|          53086|\n|          5| 56102.14432989691|             NULL|          49225|\n|          4| 89830.77570093458|             NULL|          48072|\n|          7|58944.688524590165|             NULL|          49629|\n|          2|           66428.4|             NULL|          48959|\n+-----------+------------------+-----------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "family_size_aggregations_df = spark.sql(\"\"\"\n",
    "    SELECT `Family Size`, \n",
    "           AVG(Income) AS avg_income, \n",
    "           SUM(`Loan Amount`) AS total_loan_amount, \n",
    "           MAX(Expenditure) AS max_expenditure\n",
    "    FROM challenge_data\n",
    "    GROUP BY `Family Size`\n",
    "\"\"\")\n",
    "\n",
    "family_size_aggregations_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a8ad8d1-01b0-47c7-9dcf-63a76ee68344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Complex Join Operations\").getOrCreate()\n",
    "\n",
    "#customers DataFrame\n",
    "customers_data = [\n",
    "    Row(Customer_ID=1, Name=\"Alice\", Age=34, Occupation=\"Engineer\", Country=\"USA\"),\n",
    "    Row(Customer_ID=2, Name=\"Bob\", Age=45, Occupation=\"Doctor\", Country=\"Canada\"),\n",
    "    Row(Customer_ID=3, Name=\"Charlie\", Age=29, Occupation=\"Teacher\", Country=\"UK\"),\n",
    "    Row(Customer_ID=4, Name=\"David\", Age=38, Occupation=\"Architect\", Country=\"USA\")\n",
    "]\n",
    "\n",
    "customers_df = spark.createDataFrame(customers_data)\n",
    "\n",
    "#loans DataFrame\n",
    "loans_data = [\n",
    "    Row(Loan_ID=101, Customer_ID=1, Loan_Amount=50000, Loan_Type=\"Personal\", Loan_Status=\"Approved\", Loan_Start_Date=\"2023-01-10\"),\n",
    "    Row(Loan_ID=102, Customer_ID=2, Loan_Amount=30000, Loan_Type=\"Home\", Loan_Status=\"Pending\", Loan_Start_Date=\"2023-03-15\"),\n",
    "    Row(Loan_ID=103, Customer_ID=3, Loan_Amount=20000, Loan_Type=\"Education\", Loan_Status=\"Rejected\", Loan_Start_Date=\"2023-02-20\"),\n",
    "    Row(Loan_ID=104, Customer_ID=None, Loan_Amount=15000, Loan_Type=\"Car\", Loan_Status=\"Approved\", Loan_Start_Date=\"2023-05-01\")\n",
    "]\n",
    "\n",
    "loans_df = spark.createDataFrame(loans_data)\n",
    "\n",
    "# temporary views\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "loans_df.createOrReplaceTempView(\"loans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b14828f2-9791-48bc-9319-cf9b8c67fa3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------+-----------+-----------+\n|Customer_ID|   Name|Loan_ID|Loan_Amount|Loan_Status|\n+-----------+-------+-------+-----------+-----------+\n|          1|  Alice|    101|      50000|   Approved|\n|          2|    Bob|    102|      30000|    Pending|\n|          3|Charlie|    103|      20000|   Rejected|\n+-----------+-------+-------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Perform INNER JOIN (only rows with matching Customer_ID)\n",
    "inner_join_df = spark.sql(\"\"\"\n",
    "    SELECT c.Customer_ID, c.Name, l.Loan_ID, l.Loan_Amount, l.Loan_Status\n",
    "    FROM customers c\n",
    "    INNER JOIN loans l\n",
    "    ON c.Customer_ID = l.Customer_ID\n",
    "\"\"\")\n",
    "inner_join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35a102d2-19ec-48e5-8872-75081e5a6dab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------+-----------+-----------+\n|Customer_ID|   Name|Loan_ID|Loan_Amount|Loan_Status|\n+-----------+-------+-------+-----------+-----------+\n|          1|  Alice|    101|      50000|   Approved|\n|          2|    Bob|    102|      30000|    Pending|\n|          3|Charlie|    103|      20000|   Rejected|\n|          4|  David|   NULL|       NULL|       NULL|\n+-----------+-------+-------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Perform LEFT JOIN (all customers, including those with no loan records)\n",
    "left_join_df = spark.sql(\"\"\"\n",
    "    SELECT c.Customer_ID, c.Name, l.Loan_ID, l.Loan_Amount, l.Loan_Status\n",
    "    FROM customers c\n",
    "    LEFT JOIN loans l\n",
    "    ON c.Customer_ID = l.Customer_ID\n",
    "\"\"\")\n",
    "left_join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e0c5d38-c61b-4968-b83f-336008a8d82c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------+-----------+-----------+\n|Customer_ID|   Name|Loan_ID|Loan_Amount|Loan_Status|\n+-----------+-------+-------+-----------+-----------+\n|          1|  Alice|    101|      50000|   Approved|\n|          2|    Bob|    102|      30000|    Pending|\n|          3|Charlie|    103|      20000|   Rejected|\n|       NULL|   NULL|    104|      15000|   Approved|\n+-----------+-------+-------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Perform RIGHT JOIN (all loans, including those without corresponding customer records)\n",
    "right_join_df = spark.sql(\"\"\"\n",
    "    SELECT c.Customer_ID, c.Name, l.Loan_ID, l.Loan_Amount, l.Loan_Status\n",
    "    FROM customers c\n",
    "    RIGHT JOIN loans l\n",
    "    ON c.Customer_ID = l.Customer_ID\n",
    "\"\"\")\n",
    "right_join_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2024-11-26 16:32:08",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
